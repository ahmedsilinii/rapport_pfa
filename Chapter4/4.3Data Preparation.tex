\section*{4.3 Data Preparation}

\paragraph{4.3.1 Overview\\}
To enable the use of the CLAP (Contrastive Language-Audio Pretraining) model, both audio signals and textual descriptions were preprocessed and aligned into a format suitable for joint embedding. Data preparation involved several key stages: cleaning, transformation, feature generation, and multimodal input construction.

\paragraph{4.3.2 Audio Preprocessing\\}
\begin{itemize}
    \item \textbf{Loading:} WAV audio files were loaded using \texttt{torchaudio} with consistent settings across all samples.
    \item \textbf{Resampling:} All audio signals were resampled to 48\,kHz to match the input requirements of the CLAP model.
    \item \textbf{Normalization:} Amplitude normalization was applied to reduce variability in recording loudness.
    \item \textbf{Device Filtering:} Only audio recorded using a microphone device was retained; recordings captured via stethoscope or user-recorded samples were excluded to ensure consistency and realistic ambient conditions.
\end{itemize}

\paragraph{4.3.3 Text Preprocessing\\}
\begin{itemize}
    \item \textbf{Demographic Metadata Integration:} Patient age, sex, and physical metrics were combined into a coherent textual description.
    \item \textbf{Description Generation:} A custom function generated natural language sentences, e.g., \textit{"Patient 103 is a 7-year-old female child weighing 22 kg and measuring 120 cm in height."}
    \item \textbf{Tokenization:} Descriptions were tokenized using the CLAP processor to match the format of its language encoder.
    \item \textbf{Missing Value Handling:} Patients missing critical metadata fields were assigned defaults.
\end{itemize}

\paragraph{4.3.4 Multimodal Pair Construction\\}
Each input sample consisted of a synchronized multimodal pair:
\begin{enumerate}
    \item A preprocessed 5-second audio tensor representing a single breathing cycle.
    \item A text description specific to the patient from whom the audio was collected.
\end{enumerate}
These were fed into the CLAP model to generate aligned audio-text embeddings in a shared latent space.

\paragraph{4.3.5 Label Encoding\\}
\begin{itemize}
    \item Respiratory condition labels (e.g., crackles, wheezes, normal) were derived from expert annotations per breathing cycle.
    \item Labels were encoded using \texttt{LabelEncoder} from \texttt{scikit-learn} to obtain numerical class indices.
    \item Ambiguous or unlabeled cycles were excluded to avoid introducing noise in the training process.
\end{itemize}

\paragraph{4.3.6 Dataset Splitting\\}
\begin{itemize}
    \item An official dataset split provided by the dataset curators was used to separate training and test sets.
    \item This ensured reproducibility and comparability with prior work.
    \item Patient-level disjointness was maintained: no subject appeared in both training and test sets, preventing data leakage.
\end{itemize}

\paragraph{Summary}
The preprocessing pipeline standardized and aligned audio and text inputs for CLAP-based embedding. Filtering by recording device, adherence to the official train/test split, and careful label and segmentation handling ensured high-quality inputs for downstream classification.
